{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c46094a-539e-4e32-a727-953d9dcf1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "root = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdaff0c6-4cab-4c0a-91bc-9c960109f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs_k :40\n",
      "vocabs_v :1166\n",
      "vocabs_tgt :9993\n",
      "vocabs_v + vocabs_tgt:10146\n",
      "vocabs_all:10182\n"
     ]
    }
   ],
   "source": [
    "vocabs_k=set()\n",
    "vocabs_v=set()\n",
    "with open(root+'train_input.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        pairs = np.array([pair.split(u\"￨\") for pair in line.split()])\n",
    "        vocabs_k |= set(pairs[:, 1])\n",
    "        vocabs_v |= set(pairs[:, 0])\n",
    "\n",
    "vocabs_tgt=set()\n",
    "with open(root+'train_output.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().split()\n",
    "        vocabs_tgt |= set(line)\n",
    "\n",
    "print(f'vocabs_k :{len(vocabs_k)}')\n",
    "print(f'vocabs_v :{len(vocabs_v)}')\n",
    "print(f'vocabs_tgt :{len(vocabs_tgt)}')\n",
    "print(f'vocabs_v + vocabs_tgt:{len(vocabs_v|vocabs_tgt)}')\n",
    "print(f'vocabs_all:{len(vocabs_v|vocabs_tgt|vocabs_k)}')\n",
    "\n",
    "vocabs_default=['<pad>', '<bos>', '<eos>', '<unk>', '<ent>']\n",
    "word2idx, idx2word={}, {}\n",
    "all_vocab = vocabs_default+list(vocabs_k|vocabs_v|vocabs_tgt)\n",
    "for idx, word in enumerate(all_vocab):\n",
    "    word2idx[word]=idx\n",
    "    idx2word[idx]=word\n",
    "\n",
    "vocabs = {'idx2word':idx2word, 'word2idx':word2idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a345e7f-0fb1-4476-ab06-077b14b2ca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in idx2word:\n",
    "    a = idx2word[idx]\n",
    "    b = word2idx[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50598e8-6e9f-4a4f-8c36-646e3f11278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fc42d-e680-4f1e-ba1c-4539e4c496ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(save_path):\n",
    "    vocabs = torch.load(save_path+'/'+'vocabs.pt')\n",
    "    value = vocabs['word2idx']['<unk>']\n",
    "    vocabs['word2idx']  = defaultdict(lambda: value, vocabs['word2idx'])\n",
    "\n",
    "    dataset = {'train': {'src_k':[], 'src_v':[], 'src_lengths':[], 'tgt':[], 'tgt_lengths':[], 'alignment':[], 'template':[]},\n",
    "               'valid': {'src_k':[], 'src_v':[], 'src_lengths':[], 'tgt':[], 'tgt_lengths':[], 'alignment':[], 'template':[]},\n",
    "               'test': {'src_k':[], 'src_v':[], 'src_lengths':[], 'tgt':[], 'tgt_lengths':[], 'alignment':[], 'template':[]},        \n",
    "            }\n",
    "    set_name = ['train', 'valid', 'test']\n",
    "    types = ['_input.txt', '_output.txt']\n",
    "\n",
    "\n",
    "    for name in set_name:\n",
    "        print(name)\n",
    "        input_file_path = root+name+types[0]\n",
    "        print(input_file_path)\n",
    "        with open(input_file_path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for idx, line in enumerate(lines):\n",
    "                pairs = np.array([pair.split(u\"￨\") for pair in line.split()])\n",
    "                src_k = pairs[:, 1]\n",
    "                src_v = pairs[:, 0]\n",
    "                length = len(src_k)\n",
    "\n",
    "                src_k = [vocabs['word2idx'][word] for word in src_k]\n",
    "                src_v = [vocabs['word2idx'][word] for word in src_v]                \n",
    "                # save\n",
    "                dataset[name]['src_k'].append(src_k)\n",
    "                dataset[name]['src_v'].append(src_v)\n",
    "                dataset[name]['src_lengths'].append(length)\n",
    "\n",
    "        output_file_path = root+name+types[1]\n",
    "        print(output_file_path)\n",
    "        with open(output_file_path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tgt = line.strip().split()\n",
    "                tgt += ['<eos>']\n",
    "                length = len(tgt)\n",
    "\n",
    "                tgt = [vocabs['word2idx'][word] for word in tgt]               \n",
    "                # save\n",
    "                dataset[name]['tgt_lengths'].append(length)\n",
    "                dataset[name]['tgt'].append(tgt)\n",
    "\n",
    "\n",
    "        ## DO ALIGNMENT\n",
    "        debug=False\n",
    "        text_to_number = {  'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "                            'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17, 'eighteen': 18,\n",
    "                            'nineteen': 19, 'twenty': 20, 'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70, 'eighty': 80, 'ninety': 90 \n",
    "                         }\n",
    "        with open(input_file_path, encoding='utf-8') as fi, open(output_file_path, encoding='utf-8') as fo:\n",
    "            input_lines, output_lines = fi.readlines(), fo.readlines()\n",
    "            for (input_line, output_line) in tqdm(zip(input_lines, output_lines)):\n",
    "                ## data preparation\n",
    "                tgt = output_line.strip().split()\n",
    "                tgt += ['<eos>']\n",
    "                template = tgt\n",
    "                pairs = np.array([pair.split(u\"￨\") for pair in input_line.split()])\n",
    "                src_k = pairs[:, 1]\n",
    "                src_v = pairs[:, 0] \n",
    "                alignment = [-1]*len(tgt)\n",
    "                buffers, buffer_, queue_for_second_stage = [], {'TEAM-CITY':[],'TEAM-NAME':[],'FIRST_NAME':[],'SECOND_NAME':[],'target':[]}, []\n",
    "\n",
    "                ## first stage alignment\n",
    "                for word_idx, word in enumerate(tgt):            \n",
    "                    if word in text_to_number.keys():\n",
    "                        word_ = str(text_to_number[word])\n",
    "                    else:\n",
    "                        word_ = word\n",
    "\n",
    "                    indices = [i for i, x in enumerate(src_v) if x == word_]\n",
    "                    key_name = [src_k[idx] for idx in indices]            \n",
    "                    if indices:\n",
    "                        # template information\n",
    "                        template[word_idx] = '<ent>'\n",
    "                        if debug: print(f'No.{word_idx} Word:[{word}]: indices:{indices}, key:{key_name}\\n')\n",
    "                        # add to queue_for_next_stage\n",
    "                        if len(indices) > 1: \n",
    "                            queue_for_second_stage.append([len(buffers), word_idx, indices, key_name])\n",
    "                        elif len(indices)==1:\n",
    "                            alignment[word_idx]=indices[0]\n",
    "\n",
    "                        # add target into buffer \n",
    "                        for key, index in zip(key_name, indices):\n",
    "                            if key in buffer_.keys():\n",
    "                                index = 24*(index//24+1)-1\n",
    "                                buffer_[key].append(index)\n",
    "                    else:\n",
    "                        if debug: print(f'No.{word_idx} Word:[{word}]: indices:{indices}\\n')\n",
    "\n",
    "                    if word == '.' or word == '<eos>': \n",
    "                        # merge target\n",
    "                        for first, second in [(set(buffer_['TEAM-CITY']), set(buffer_['TEAM-NAME']) ), (set(buffer_['FIRST_NAME']), set(buffer_['SECOND_NAME']))]:\n",
    "                            if len(first)>0 and len(second)>0:\n",
    "                                buffer_['target']+=list(first&second)\n",
    "                            elif (len(first)>0 and len(second)==0) or (len(first)==0 and len(second)>0):\n",
    "                                buffer_['target']+=list(first|second)             \n",
    "\n",
    "                        if debug: print(\"=\"*100)\n",
    "                        if debug: print(f'length of queue_for_second_stage: {len(queue_for_second_stage)}')\n",
    "                        if debug: print(f'buffer: {buffer_}')\n",
    "                        if debug: print(\"=\"*100)\n",
    "\n",
    "                        # initial\n",
    "                        buffers.append(buffer_)\n",
    "                        buffer_={'TEAM-CITY':[],'TEAM-NAME':[],'FIRST_NAME':[],'SECOND_NAME':[],'target':[]}\n",
    "\n",
    "                ## second stage alignment\n",
    "\n",
    "                queue_for_third_stage =[]\n",
    "                for buffer_idx, word_idx, indices, keys in queue_for_second_stage:\n",
    "                    indices_target,keys_target=[],[]\n",
    "                    for idx, key in zip(indices, keys):\n",
    "                        if 24*(idx//24+1)-1 in buffers[buffer_idx]['target']: \n",
    "                            indices_target.append(idx)\n",
    "                            keys_target.append(key)\n",
    "\n",
    "                    ## alignment\n",
    "                    if len(indices_target) == 1: alignment[word_idx]=indices_target[0]\n",
    "                    elif len(indices_target) > 1: queue_for_third_stage.append([buffer_idx, word_idx, indices_target, keys_target])\n",
    "\n",
    "                    if debug: print(f'buffer_idx: {buffer_idx}, word_idx:{word_idx}, indices:{indices}')\n",
    "                    if debug: print(f'buffer_idx: {buffer_idx}, buffer:{buffers[buffer_idx]}')\n",
    "                    if debug: print(f'merged target:{indices_target} key_target:{keys_target}\\n')        \n",
    "\n",
    "\n",
    "                ## third stage alignment\n",
    "                for buffer_idx, word_idx, indices, keys in queue_for_third_stage:\n",
    "                    alignment[word_idx]=indices[0]\n",
    "                \n",
    "                # save\n",
    "                dataset[name]['alignment'].append(alignment)\n",
    "                dataset[name]['template'].append([vocabs['word2idx'][word] for word in template])\n",
    "\n",
    "    torch.save(dataset, save_path+'/'+'dataset.pt')\n",
    "    return \n",
    "\n",
    "def build(save_path):\n",
    "    build_vocab(save_path)\n",
    "    build_dataset(save_path)\n",
    "    return\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f970725-2ba7-4d09-b64c-bc0158f6a667",
   "metadata": {},
   "source": [
    "## real alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dd7918-1ce5-431b-805c-94d8e04b9b45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "name = 'test'\n",
    "types = ['_input.txt', '_output.txt']\n",
    "input_file_path = root+name+types[0]\n",
    "output_file_path = root+name+types[1]\n",
    "\n",
    "## DO ALIGNMENT\n",
    "debug=False\n",
    "text_to_number = {  'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,\n",
    "                    'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15, 'sixteen': 16, 'seventeen': 17, 'eighteen': 18,\n",
    "                    'nineteen': 19, 'twenty': 20, 'thirty': 30, 'forty': 40, 'fifty': 50, 'sixty': 60, 'seventy': 70, 'eighty': 80, 'ninety': 90 \n",
    "                 }\n",
    "with open(input_file_path, encoding='utf-8') as fi, open(output_file_path, encoding='utf-8') as fo:\n",
    "    input_lines, output_lines = fi.readlines(), fo.readlines()\n",
    "    for (input_line, output_line) in tqdm(zip(input_lines, output_lines)):\n",
    "        ## data preparation\n",
    "        tgt = output_line.strip().split()\n",
    "        tgt += ['<eos>']\n",
    "        template = tgt\n",
    "        pairs = np.array([pair.split(u\"￨\") for pair in input_line.split()])\n",
    "        src_k = pairs[:, 1]\n",
    "        src_v = pairs[:, 0] \n",
    "        alignment = [-1]*len(tgt)\n",
    "        buffers, buffer_, queue_for_second_stage = [], {'TEAM-CITY':[],'TEAM-NAME':[],'FIRST_NAME':[],'SECOND_NAME':[],'target':[]}, []\n",
    "\n",
    "        ## first stage alignment\n",
    "        for word_idx, word in enumerate(tgt):            \n",
    "            if word in text_to_number.keys():\n",
    "                word_ = str(text_to_number[word])\n",
    "            else:\n",
    "                word_ = word\n",
    "\n",
    "            indices = [i for i, x in enumerate(src_v) if x == word_]\n",
    "            key_name = [src_k[idx] for idx in indices]            \n",
    "            if indices:\n",
    "                # template information\n",
    "                template[word_idx] = '<ent>'\n",
    "                if debug: print(f'No.{word_idx} Word:[{word}]: indices:{indices}, key:{key_name}\\n')\n",
    "                # add to queue_for_next_stage\n",
    "                if len(indices) > 1: \n",
    "                    queue_for_second_stage.append([len(buffers), word_idx, indices, key_name])\n",
    "                elif len(indices)==1:\n",
    "                    alignment[word_idx]=indices[0]\n",
    "\n",
    "                # add target into buffer \n",
    "                for key, index in zip(key_name, indices):\n",
    "                    if key in buffer_.keys():\n",
    "                        index = 24*(index//24+1)-1\n",
    "                        buffer_[key].append(index)\n",
    "            else:\n",
    "                if debug: print(f'No.{word_idx} Word:[{word}]: indices:{indices}\\n')\n",
    "\n",
    "            if word == '.' or word == '<eos>': \n",
    "                # merge target\n",
    "                for first, second in [(set(buffer_['TEAM-CITY']), set(buffer_['TEAM-NAME']) ), (set(buffer_['FIRST_NAME']), set(buffer_['SECOND_NAME']))]:\n",
    "                    if len(first)>0 and len(second)>0:\n",
    "                        buffer_['target']+=list(first&second)\n",
    "                    elif (len(first)>0 and len(second)==0) or (len(first)==0 and len(second)>0):\n",
    "                        buffer_['target']+=list(first|second)             \n",
    "\n",
    "                if debug: print(\"=\"*100)\n",
    "                if debug: print(f'length of queue_for_second_stage: {len(queue_for_second_stage)}')\n",
    "                if debug: print(f'buffer: {buffer_}')\n",
    "                if debug: print(\"=\"*100)\n",
    "\n",
    "                # initial\n",
    "                buffers.append(buffer_)\n",
    "                buffer_={'TEAM-CITY':[],'TEAM-NAME':[],'FIRST_NAME':[],'SECOND_NAME':[],'target':[]}\n",
    "\n",
    "        ## second stage alignment\n",
    "\n",
    "        queue_for_third_stage =[]\n",
    "        for buffer_idx, word_idx, indices, keys in queue_for_second_stage:\n",
    "            indices_target,keys_target=[],[]\n",
    "            for idx, key in zip(indices, keys):\n",
    "                if 24*(idx//24+1)-1 in buffers[buffer_idx]['target']: \n",
    "                    indices_target.append(idx)\n",
    "                    keys_target.append(key)\n",
    "\n",
    "            ## alignment\n",
    "            if len(indices_target) == 1: alignment[word_idx]=indices_target[0]\n",
    "            elif len(indices_target) > 1: queue_for_third_stage.append([buffer_idx, word_idx, indices_target, keys_target])\n",
    "\n",
    "            if debug: print(f'buffer_idx: {buffer_idx}, word_idx:{word_idx}, indices:{indices}')\n",
    "            if debug: print(f'buffer_idx: {buffer_idx}, buffer:{buffers[buffer_idx]}')\n",
    "            if debug: print(f'merged target:{indices_target} key_target:{keys_target}\\n')        \n",
    "\n",
    "\n",
    "        ## third stage alignment\n",
    "        for buffer_idx, word_idx, indices, keys in queue_for_third_stage:\n",
    "            alignment[word_idx]=indices[0]\n",
    "\n",
    "        # save\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dec1fb-dca8-4f8e-98f5-a17481cc3f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
